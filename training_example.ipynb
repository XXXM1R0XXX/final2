{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVnWB-3HjwHn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#подгрузка датасета\n",
        "%%capture\n",
        "!gdown 1Rt0I7Svrx77tFMCsNubEQ-cDY8hD-iCk\n",
        "!gdown 1GWyzUaz_mOwYDbLuopjroIcfngjSJWkD\n",
        "!unzip r_peaks.zip"
      ],
      "metadata": {
        "id": "zrKNA6OQxNlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#импорт\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict, OrderedDict"
      ],
      "metadata": {
        "id": "RXDm2YGFkh9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#разделение на обучаемый класс\n",
        "labels = pd.read_csv(\"train_val_labels.csv\")\n",
        "target_class = 0\n",
        "left_classes = [i for i in labels.result_class.unique() if i != target_class]\n",
        "num_others = (len(labels[labels.result_class == target_class]) * 2) // 15\n",
        "data = labels[labels.result_class == target_class]\n",
        "data.loc[:, [\"result_class\"]] = 1\n",
        "data.index = range(0, len(data))\n",
        "for cur_class in left_classes:\n",
        "  cur_class_data = labels[(labels.result_class == cur_class)]\n",
        "  cur_class_data = cur_class_data[~cur_class_data.record_name.isin(labels[labels.result_class != cur_class].record_name)]\n",
        "  cur_frame = cur_class_data.sample(n=min(len(cur_class_data), num_others))\n",
        "  cur_frame.loc[:, [\"result_class\"]] = 0\n",
        "  data = pd.concat([data, cur_frame], axis=0)"
      ],
      "metadata": {
        "id": "TeFOFly7_a4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "CJtOW5cAxJEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EcgPTBDataset(Dataset):\n",
        "    def __init__(self, labels, path='/'):\n",
        "        self.x_paths = [labels.iloc[i, 0] for i in range(len(labels))]\n",
        "        self.labels = [labels.iloc[i, 1] for i in range(len(labels))]\n",
        "        self.path = path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        hr = torch.tensor(np.load(self.path + self.x_paths[idx] + '.npy'))[None, :, :]\n",
        "\n",
        "        target = self.labels[idx]\n",
        "\n",
        "        return hr, target"
      ],
      "metadata": {
        "id": "ff8S4sfZk7b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#создаём датасет\n",
        "ptb_set = EcgPTBDataset(data, path=\"/content/r_peaks/signals/\")\n",
        "\n",
        "# train_data, valid_data = train_test_split(ptb_set, test_size=0.1)\n",
        "valid_data, train_data = random_split(ptb_set, lengths=[0.1, 0.9])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=1)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=1)"
      ],
      "metadata": {
        "id": "1UEWfz3elL0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ECGNET"
      ],
      "metadata": {
        "id": "1V_ARXu4kbny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Класс модели\n",
        "\n",
        "class ECGNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ECGNet, self).__init__()\n",
        "    #layer1\n",
        "    self.layer1_conv2d = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1, 25), stride=(1, 2), bias=True)\n",
        "\n",
        "\n",
        "    #layer2\n",
        "    self.layer2_conv2d = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(1, 15), stride=(1, 1), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(1, 15), stride=(1, 2),  bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(1, 15), stride=(1, 1), bias=True)),\n",
        "    ]))\n",
        "    self.layer2_seModule = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    #layer3\n",
        "    self.layer3_conv2d_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer3_conv2d_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer3_conv2d_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    #layer4\n",
        "    self.layer4_conv1d_short_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=3, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=3, stride=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=3, stride=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=3, stride=2, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=3, stride=2, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_short_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=5, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=5, stride=2, padding=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=5, stride=2, padding=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=5, stride=1, padding=2, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=5, stride=2, padding=1, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_short_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=7, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=7, stride=2, padding=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=7, stride=2, padding=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=7, stride=1, padding=3, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=7, stride=2, padding=2, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer5_avg_pool1 = nn.AvgPool1d(kernel_size=10)\n",
        "    self.layer5_avg_pool2 = nn.AvgPool1d(kernel_size=10)\n",
        "    self.layer5_avg_pool3 = nn.AvgPool1d(kernel_size=10)\n",
        "\n",
        "    self.fc = nn.Sequential(OrderedDict([\n",
        "        (\"ln1\", nn.Linear(1152, 288)),\n",
        "        (\"dp\", nn.Dropout(p=0.2)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"ln2\", nn.Linear(288, 1)),\n",
        "        (\"sigmoid\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    #layer1\n",
        "    x = self.layer1_conv2d(x)\n",
        "\n",
        "    #layer2\n",
        "    x = self.layer2_conv2d(x)\n",
        "    u = x\n",
        "    x = x.view(x.size(0), x.size(1), -1).mean(-1).view(x.size(0), x.size(1), 1, 1)\n",
        "    x = self.layer2_seModule(x)\n",
        "    x = u * x\n",
        "\n",
        "    #layer3\n",
        "    x1 = self.layer3_conv2d_block1(x)\n",
        "    u1 = x1\n",
        "    x1 = x1.view(x1.size(0), x1.size(1), -1).mean(-1).view(x1.size(0), x1.size(1), 1, 1)\n",
        "    x1 = self.layer3_seModule_block1(x1)\n",
        "    x1 = u1 * x1\n",
        "\n",
        "    x2 = self.layer3_conv2d_block2(x)\n",
        "    u2 = x2\n",
        "    x2 = x2.view(x2.size(0), x2.size(1), -1).mean(-1).view(x2.size(0), x2.size(1), 1, 1)\n",
        "    x2 = self.layer3_seModule_block2(x2)\n",
        "    x2 = u2 * x2\n",
        "\n",
        "    x3 = self.layer3_conv2d_block3(x)\n",
        "    u3 = x3\n",
        "    x3 = x3.view(x3.size(0), x3.size(1), -1).mean(-1).view(x3.size(0), x3.size(1), 1, 1)\n",
        "    x3 = self.layer3_seModule_block3(x3)\n",
        "    x3 = u3 * x3\n",
        "\n",
        "    #layer4\n",
        "    x1 = torch.flatten(x1, start_dim=1, end_dim=2)\n",
        "    x2 = torch.flatten(x2, start_dim=1, end_dim=2)\n",
        "    x3 = torch.flatten(x3, start_dim=1, end_dim=2)\n",
        "\n",
        "\n",
        "    x1_short = self.layer4_conv1d_short_block1(x1)\n",
        "\n",
        "    x1 = self.layer4_conv1d_block1(x1)\n",
        "    u1 = x1\n",
        "    x1 = x1.view(x1.size(0), x1.size(1), -1).mean(-1).view(x1.size(0), x1.size(1), 1, 1).flatten(2, 3)\n",
        "    x1 = self.layer4_seModule_block1(x1)\n",
        "    x1 = u1 * x1\n",
        "    x1 = x1 + x1_short\n",
        "\n",
        "    x2_short = self.layer4_conv1d_short_block2(x2)\n",
        "\n",
        "    x2 = self.layer4_conv1d_block2(x2)\n",
        "    u2 = x2\n",
        "    x2 = x2.view(x2.size(0), x2.size(1), -1).mean(-1).view(x2.size(0), x2.size(1), 1, 1).flatten(2, 3)\n",
        "    x2 = self.layer4_seModule_block2(x2)\n",
        "    x2 = u2 * x2\n",
        "    x2 = x2 + x2_short\n",
        "\n",
        "    x3_short = self.layer4_conv1d_short_block3(x3)\n",
        "\n",
        "    x3 = self.layer4_conv1d_block3(x3)\n",
        "    u3 = x3\n",
        "    x3 = x3.view(x3.size(0), x3.size(1), -1).mean(-1).view(x3.size(0), x3.size(1), 1, 1).flatten(2, 3)\n",
        "    x3 = self.layer4_seModule_block3(x3)\n",
        "    x3 = u3 * x3\n",
        "    x3 = x3 + x3_short\n",
        "\n",
        "    x1 = self.layer5_avg_pool1(x1)\n",
        "    x2 = self.layer5_avg_pool2(x2)\n",
        "    x3 = self.layer5_avg_pool3(x3)\n",
        "\n",
        "    x = torch.cat((x1, x2, x3), dim=1).flatten(1)\n",
        "\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "r1XSNivRkaEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "3mC8KeqmxMsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#рукуписные метрики\n",
        "def calculate_accuracy(output, target):\n",
        "    train_accuracy = torch.sum(target == output) / len(target)\n",
        "    return train_accuracy\n",
        "\n",
        "def calculate_f1(preds, labels):\n",
        "    tp = torch.sum(preds[labels == preds] == 1)\n",
        "    preds_p = torch.sum(preds == 1)\n",
        "    labels_p = torch.sum(labels == 1)\n",
        "    recall = (tp / labels_p if labels_p != 0 else 0)\n",
        "    precision = (tp / preds_p if preds_p != 0 else 0)\n",
        "    if recall + precision == 0: return 0\n",
        "    return (2 * recall * precision) / (recall + precision)\n",
        "\n",
        "class MetricMonitor:\n",
        "    def __init__(self, float_precision=3):\n",
        "        self.float_precision = float_precision\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
        "\n",
        "    def update(self, metric_name, val):\n",
        "        metric = self.metrics[metric_name]\n",
        "\n",
        "        metric[\"val\"] += val\n",
        "        metric[\"count\"] += 1\n",
        "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \" | \".join(\n",
        "            [\n",
        "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
        "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
        "                )\n",
        "                for (metric_name, metric) in self.metrics.items()\n",
        "            ]\n",
        "        )"
      ],
      "metadata": {
        "id": "iI2rCig7NtIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Valid part"
      ],
      "metadata": {
        "id": "w7gMEMVYxPy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#обучение\n",
        "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
        "    metric_monitor = MetricMonitor(float_precision=4)\n",
        "    model.train()\n",
        "    stream = tqdm(train_loader)\n",
        "    for i, batch in enumerate(stream, start=1):\n",
        "        x_batch, y_batch = batch\n",
        "        y_batch = y_batch.to(device, non_blocking=True)\n",
        "        x_batch = x_batch.to(device, non_blocking=True)\n",
        "        output = model(x_batch.float()).view(1, -1)[0]\n",
        "        loss = criterion(output, y_batch.float())\n",
        "        output = (output > 0.5).to(torch.int32)\n",
        "        accuracy = calculate_accuracy(output, y_batch)\n",
        "        f1 = calculate_f1(output, y_batch)\n",
        "        metric_monitor.update(\"Loss\", loss)\n",
        "        metric_monitor.update(\"Accuracy\", accuracy)\n",
        "        metric_monitor.update(\"F1\", f1)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stream.set_description(\n",
        "            \"Epoch: {epoch}. Train.  {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "        )"
      ],
      "metadata": {
        "id": "PdFtCcuhn9k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#валидация\n",
        "def validate(val_loader, model, criterion, epoch, device):\n",
        "    metric_monitor = MetricMonitor(float_precision=4)\n",
        "    model.eval()\n",
        "    stream = tqdm(val_loader)\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(stream, start=1):\n",
        "            x_batch, y_batch = batch\n",
        "            y_batch = y_batch.to(device, non_blocking=True)\n",
        "            x_batch = x_batch.to(device, non_blocking=True)\n",
        "            output = model(x_batch.float()).view(1, -1)[0]\n",
        "            loss = criterion(output, y_batch.float())\n",
        "            output = (output > 0.5).to(torch.int32)\n",
        "            accuracy = calculate_accuracy(output, y_batch)\n",
        "            f1 = calculate_f1(output, y_batch)\n",
        "            metric_monitor.update(\"Loss\", loss)\n",
        "            metric_monitor.update(\"Accuracy\", accuracy)\n",
        "            metric_monitor.update(\"F1\", f1)\n",
        "            stream.set_description(\n",
        "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "            )\n",
        "    return metric_monitor.metrics[\"F1\"][\"avg\"], metric_monitor.metrics[\"Accuracy\"][\"avg\"], metric_monitor.metrics[\"Loss\"][\"avg\"]"
      ],
      "metadata": {
        "id": "ACGdfJ3loAJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#параметры обучения\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ECGNet()\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate = 3e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "lMyWw32WmVGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#по надобности подключение к Wandb для отслеживания метрик во время обучения\n",
        "!wandb login\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"big_calls\",\n",
        "\n",
        "    config={\n",
        "        \"architecture\": \"ecg_net\",\n",
        "        \"dataset\": \"ecgs\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "iO5EGQt51uk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#подключение к гугл диску\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjYwjqn3Pgbq",
        "outputId": "84ec08a2-4111-49e4-a59c-ac1157a1fc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#запускаем конвейер обучения\n",
        "num_epochs = 10\n",
        "max_f1 = 0.8\n",
        "for epoch in range(num_epochs):\n",
        "  train(train_loader, model, loss_fn, optimizer, epoch, device)\n",
        "  f1_v, acc_v, loss_v = validate(valid_loader, model, loss_fn, epoch, device)\n",
        "  scheduler.step(f1_v)\n",
        "  if f1_v > max_f1:\n",
        "    max_f1 = f1_v\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/colabs/ALT+F4/AIIJC/models/bigcalls/IRBBB/{f1_v}.pth')"
      ],
      "metadata": {
        "id": "KqDyEpvez6eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тюним"
      ],
      "metadata": {
        "id": "7dqXweo6xVg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### В будущем будет реализован тюнинг моделей"
      ],
      "metadata": {
        "id": "guTdhtn7yaPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def objective(trial):\n",
        "#     lr_base = trial.suggest_categorical(\"lr_base\", [2e-4, 3e-4, 2e-5])\n",
        "#     optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"Adagrad\", \"RMSprop\"])\n",
        "\n",
        "#     device = torch.device(\"cuda\")\n",
        "#     model = create_model(trial).to(device)\n",
        "#     criterion = nn.CrossEntropyLoss().to(device)\n",
        "#     optimizer = getattr(torch.optim, optimizer)(model.parameters(), lr=lr_base)\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "#     train_loader = DataLoader(\n",
        "#         train_dset, batch_size=64, shuffle=True, num_workers=1, pin_memory=True,\n",
        "#     )\n",
        "#     val_loader = DataLoader(\n",
        "#         valid_dset, batch_size=64, shuffle=False, num_workers=1, pin_memory=True,\n",
        "#     )\n",
        "\n",
        "#     for epoch in range(1, 21):\n",
        "#         train(train_loader, model, criterion, optimizer, epoch, device)\n",
        "#         f1, acc = validate(val_loader, model, criterion, epoch, device)\n",
        "\n",
        "#         scheduler.step(acc)\n",
        "\n",
        "#         trial.report(f1, epoch)\n",
        "\n",
        "#         if trial.should_prune():\n",
        "#             raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "#     return f1"
      ],
      "metadata": {
        "id": "fp90SJuUoDF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# from IPython.display import clear_output\n",
        "\n",
        "# num_epochs = 1\n",
        "\n",
        "# def callback(study, trial):\n",
        "#     global best_model\n",
        "#     if study.best_trial == trial:\n",
        "#         best_model = model_image\n",
        "\n",
        "# def clean_stream(study, trial):\n",
        "#     global num_epochs\n",
        "#     clear_output(wait=True)\n",
        "#     num_epochs += 1\n",
        "#     print(num_epochs)\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=30, callbacks=[callback, clean_stream])\n",
        "\n",
        "# end_time = time.time()\n",
        "# took_time =  end_time - start_time"
      ],
      "metadata": {
        "id": "yCBq6EekoHlP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}